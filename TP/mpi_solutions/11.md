

## Key Concepts Explained:

### 1. **Block Distribution Strategy:**
- **Equal Partitioning**: The range `[1, upper_bound]` is divided into `size` contiguous blocks
- **Example**: If `upper_bound=100` and `size=4`:
  - Process 0: numbers 1-25
  - Process 1: numbers 26-50  
  - Process 2: numbers 51-75
  - Process 3: numbers 76-100
- **Edge Case Handling**: Last process takes any remaining numbers if not perfectly divisible

### 2. **MPI Collective Operations:**
- **`comm.Barrier()`**: Synchronizes all processes, ensuring timing measurements are consistent
- **`comm.Reduce()`**: 
  - **Operation**: `MPI.MAX` finds the maximum value across all processes
  - **Root Process**: Only rank 0 receives the final result
  - **Efficiency**: More scalable than gathering all data to root and then computing max

### 3. **Timing Strategy:**
- **Total Time**: Measures from start to finish (computation + communication)
- **Local Compute Time**: Measures only the loop computation, excluding MPI communication
- **Purpose**: Helps identify if communication overhead is significant compared to computation

### 4. **Workload Characteristics:**
- **Computational Cost**: `nb_primes(n)` has O(n) complexity for each number
- **Load Imbalance**: Larger numbers take longer to process, but block distribution assumes uniform cost
- **Memory Efficiency**: Only local maximum is communicated, not all divisor counts

--------------------------------



## Key Concepts Explained:

### 1. **Round Robin (Cyclic) Distribution Strategy:**
- **Interleaved Assignment**: Numbers are distributed across processes in a cyclic manner
- **Mathematical Pattern**: Each process `rank` processes numbers: `rank + 1, rank + 1 + size, rank + 1 + 2*size, ...`
- **Example**: If `upper_bound=12` and `size=4`:
  - Process 0: numbers 1, 5, 9
  - Process 1: numbers 2, 6, 10  
  - Process 2: numbers 3, 7, 11
  - Process 3: numbers 4, 8, 12

### 2. **Range Function Parameters:**
- **Start**: `rank + 1` (process 0 starts at 1, process 1 at 2, etc.)
- **Stop**: `upper_bound + 1` (inclusive upper bound)
- **Step**: `size` (skip to the next number assigned to this process)

### 3. **Load Balancing Advantage:**
- **Uniform Distribution**: Each process gets a mix of small and large numbers
- **Small Numbers**: Quick to compute (fewer divisors to check)
- **Large Numbers**: Slow to compute (more divisors to check)
- **Natural Balance**: Over time, all processes do similar amounts of work

### 4. **Comparison with Block Distribution:**

| Aspect | Block Distribution | Round Robin Distribution |
|--------|-------------------|--------------------------|
| **Work Assignment** | Contiguous blocks | Interleaved numbers |
| **Load Balance** | Poor (last process gets largest numbers) | Excellent (mixed sizes) |
| **Cache Performance** | Good (spatial locality) | Poor (jumping in memory) |
| **Implementation** | More complex range calculation | Simple loop with stride |

### 5. **Performance Characteristics:**
- **Communication**: Same as block approach - only local maximum is communicated
- **Computation**: Better load balancing, especially for problems where computational cost varies with input size
- **Scalability**: Excellent for irregular workloads where some inputs are much more expensive than others

### 6. **When to Use Round Robin:**
- **Irregular Workloads**: When processing time varies significantly between different inputs
- **Unknown Cost**: When you cannot predict which inputs will be computationally expensive
- **Dynamic Systems**: When workload characteristics may change during execution

### 7. **Potential Drawbacks:**
- **Cache Performance**: Poor spatial locality as processes jump through memory
- **Overhead**: Slightly more complex addressing, but negligible for this problem
- **Synchronization**: Still need barrier for accurate timing measurements